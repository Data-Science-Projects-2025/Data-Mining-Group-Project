{
  "best_metric": 0.6232215855303119,
  "best_model_checkpoint": "./results-distilbert-unbalanced\\checkpoint-20782",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 20782,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0048118564142046,
      "grad_norm": 3.3181238174438477,
      "learning_rate": 4.9919802393096595e-05,
      "loss": 1.0744,
      "step": 100
    },
    {
      "epoch": 0.0096237128284092,
      "grad_norm": 6.54026985168457,
      "learning_rate": 4.983960478619319e-05,
      "loss": 0.9769,
      "step": 200
    },
    {
      "epoch": 0.014435569242613801,
      "grad_norm": 3.0853939056396484,
      "learning_rate": 4.975940717928977e-05,
      "loss": 0.8975,
      "step": 300
    },
    {
      "epoch": 0.0192474256568184,
      "grad_norm": 5.359822750091553,
      "learning_rate": 4.9679209572386365e-05,
      "loss": 0.9438,
      "step": 400
    },
    {
      "epoch": 0.024059282071023,
      "grad_norm": 5.2951202392578125,
      "learning_rate": 4.959901196548295e-05,
      "loss": 0.8895,
      "step": 500
    },
    {
      "epoch": 0.028871138485227602,
      "grad_norm": 3.1492185592651367,
      "learning_rate": 4.951881435857954e-05,
      "loss": 0.9034,
      "step": 600
    },
    {
      "epoch": 0.0336829948994322,
      "grad_norm": 4.241701602935791,
      "learning_rate": 4.943861675167613e-05,
      "loss": 0.8872,
      "step": 700
    },
    {
      "epoch": 0.0384948513136368,
      "grad_norm": 4.5799407958984375,
      "learning_rate": 4.935841914477272e-05,
      "loss": 0.9051,
      "step": 800
    },
    {
      "epoch": 0.0433067077278414,
      "grad_norm": 2.998889684677124,
      "learning_rate": 4.927822153786931e-05,
      "loss": 0.8884,
      "step": 900
    },
    {
      "epoch": 0.048118564142046,
      "grad_norm": 4.9158148765563965,
      "learning_rate": 4.9198023930965905e-05,
      "loss": 0.8968,
      "step": 1000
    },
    {
      "epoch": 0.0529304205562506,
      "grad_norm": 3.1507492065429688,
      "learning_rate": 4.91178263240625e-05,
      "loss": 0.8351,
      "step": 1100
    },
    {
      "epoch": 0.057742276970455204,
      "grad_norm": 3.238130807876587,
      "learning_rate": 4.903762871715908e-05,
      "loss": 0.8637,
      "step": 1200
    },
    {
      "epoch": 0.0625541333846598,
      "grad_norm": 15.941182136535645,
      "learning_rate": 4.8957431110255675e-05,
      "loss": 0.868,
      "step": 1300
    },
    {
      "epoch": 0.0673659897988644,
      "grad_norm": 4.516543865203857,
      "learning_rate": 4.887723350335226e-05,
      "loss": 0.8236,
      "step": 1400
    },
    {
      "epoch": 0.072177846213069,
      "grad_norm": 6.629319190979004,
      "learning_rate": 4.879703589644885e-05,
      "loss": 0.8831,
      "step": 1500
    },
    {
      "epoch": 0.0769897026272736,
      "grad_norm": 4.610666275024414,
      "learning_rate": 4.871683828954544e-05,
      "loss": 0.8769,
      "step": 1600
    },
    {
      "epoch": 0.0818015590414782,
      "grad_norm": 4.2443389892578125,
      "learning_rate": 4.863664068264203e-05,
      "loss": 0.8751,
      "step": 1700
    },
    {
      "epoch": 0.0866134154556828,
      "grad_norm": 1.9284099340438843,
      "learning_rate": 4.855644307573862e-05,
      "loss": 0.8463,
      "step": 1800
    },
    {
      "epoch": 0.0914252718698874,
      "grad_norm": 3.7529802322387695,
      "learning_rate": 4.8476245468835215e-05,
      "loss": 0.8492,
      "step": 1900
    },
    {
      "epoch": 0.096237128284092,
      "grad_norm": 3.748781204223633,
      "learning_rate": 4.839604786193181e-05,
      "loss": 0.8514,
      "step": 2000
    },
    {
      "epoch": 0.10104898469829661,
      "grad_norm": 4.8538899421691895,
      "learning_rate": 4.831585025502839e-05,
      "loss": 0.8676,
      "step": 2100
    },
    {
      "epoch": 0.1058608411125012,
      "grad_norm": 2.898800849914551,
      "learning_rate": 4.8235652648124985e-05,
      "loss": 0.8609,
      "step": 2200
    },
    {
      "epoch": 0.1106726975267058,
      "grad_norm": 4.969940185546875,
      "learning_rate": 4.815545504122157e-05,
      "loss": 0.8855,
      "step": 2300
    },
    {
      "epoch": 0.11548455394091041,
      "grad_norm": 10.246382713317871,
      "learning_rate": 4.807525743431816e-05,
      "loss": 0.8154,
      "step": 2400
    },
    {
      "epoch": 0.120296410355115,
      "grad_norm": 3.4493794441223145,
      "learning_rate": 4.799505982741475e-05,
      "loss": 0.8814,
      "step": 2500
    },
    {
      "epoch": 0.1251082667693196,
      "grad_norm": 3.2403972148895264,
      "learning_rate": 4.791486222051134e-05,
      "loss": 0.8505,
      "step": 2600
    },
    {
      "epoch": 0.1299201231835242,
      "grad_norm": 6.041615009307861,
      "learning_rate": 4.783466461360793e-05,
      "loss": 0.8684,
      "step": 2700
    },
    {
      "epoch": 0.1347319795977288,
      "grad_norm": 2.8988795280456543,
      "learning_rate": 4.7754467006704525e-05,
      "loss": 0.8242,
      "step": 2800
    },
    {
      "epoch": 0.1395438360119334,
      "grad_norm": 3.7418558597564697,
      "learning_rate": 4.767426939980112e-05,
      "loss": 0.8505,
      "step": 2900
    },
    {
      "epoch": 0.144355692426138,
      "grad_norm": 9.785783767700195,
      "learning_rate": 4.75940717928977e-05,
      "loss": 0.8647,
      "step": 3000
    },
    {
      "epoch": 0.1491675488403426,
      "grad_norm": 3.831989288330078,
      "learning_rate": 4.7513874185994295e-05,
      "loss": 0.8519,
      "step": 3100
    },
    {
      "epoch": 0.1539794052545472,
      "grad_norm": 3.2272350788116455,
      "learning_rate": 4.743367657909088e-05,
      "loss": 0.8453,
      "step": 3200
    },
    {
      "epoch": 0.1587912616687518,
      "grad_norm": 4.521717071533203,
      "learning_rate": 4.735347897218747e-05,
      "loss": 0.8829,
      "step": 3300
    },
    {
      "epoch": 0.1636031180829564,
      "grad_norm": 5.107012748718262,
      "learning_rate": 4.727328136528406e-05,
      "loss": 0.8367,
      "step": 3400
    },
    {
      "epoch": 0.168414974497161,
      "grad_norm": 9.642171859741211,
      "learning_rate": 4.719308375838065e-05,
      "loss": 0.8772,
      "step": 3500
    },
    {
      "epoch": 0.1732268309113656,
      "grad_norm": 4.093352317810059,
      "learning_rate": 4.711288615147724e-05,
      "loss": 0.8193,
      "step": 3600
    },
    {
      "epoch": 0.1780386873255702,
      "grad_norm": 5.783783435821533,
      "learning_rate": 4.7032688544573835e-05,
      "loss": 0.8078,
      "step": 3700
    },
    {
      "epoch": 0.1828505437397748,
      "grad_norm": 8.738463401794434,
      "learning_rate": 4.695249093767043e-05,
      "loss": 0.8423,
      "step": 3800
    },
    {
      "epoch": 0.1876624001539794,
      "grad_norm": 2.1330482959747314,
      "learning_rate": 4.687229333076701e-05,
      "loss": 0.8502,
      "step": 3900
    },
    {
      "epoch": 0.192474256568184,
      "grad_norm": 4.207923889160156,
      "learning_rate": 4.6792095723863605e-05,
      "loss": 0.8586,
      "step": 4000
    },
    {
      "epoch": 0.1972861129823886,
      "grad_norm": 4.556021690368652,
      "learning_rate": 4.671189811696019e-05,
      "loss": 0.8458,
      "step": 4100
    },
    {
      "epoch": 0.20209796939659322,
      "grad_norm": 6.590099811553955,
      "learning_rate": 4.663170051005678e-05,
      "loss": 0.8247,
      "step": 4200
    },
    {
      "epoch": 0.2069098258107978,
      "grad_norm": 4.644912242889404,
      "learning_rate": 4.655150290315337e-05,
      "loss": 0.8338,
      "step": 4300
    },
    {
      "epoch": 0.2117216822250024,
      "grad_norm": 7.252438068389893,
      "learning_rate": 4.647130529624996e-05,
      "loss": 0.8316,
      "step": 4400
    },
    {
      "epoch": 0.21653353863920702,
      "grad_norm": 2.6984457969665527,
      "learning_rate": 4.639110768934655e-05,
      "loss": 0.8049,
      "step": 4500
    },
    {
      "epoch": 0.2213453950534116,
      "grad_norm": 4.793722629547119,
      "learning_rate": 4.6310910082443145e-05,
      "loss": 0.8345,
      "step": 4600
    },
    {
      "epoch": 0.2261572514676162,
      "grad_norm": 3.8375959396362305,
      "learning_rate": 4.623071247553974e-05,
      "loss": 0.8343,
      "step": 4700
    },
    {
      "epoch": 0.23096910788182082,
      "grad_norm": 8.786378860473633,
      "learning_rate": 4.615051486863632e-05,
      "loss": 0.8276,
      "step": 4800
    },
    {
      "epoch": 0.2357809642960254,
      "grad_norm": 6.116945743560791,
      "learning_rate": 4.6070317261732915e-05,
      "loss": 0.8359,
      "step": 4900
    },
    {
      "epoch": 0.24059282071023,
      "grad_norm": 2.8557682037353516,
      "learning_rate": 4.59901196548295e-05,
      "loss": 0.829,
      "step": 5000
    },
    {
      "epoch": 0.24540467712443462,
      "grad_norm": 4.537651538848877,
      "learning_rate": 4.590992204792609e-05,
      "loss": 0.8287,
      "step": 5100
    },
    {
      "epoch": 0.2502165335386392,
      "grad_norm": 3.0599589347839355,
      "learning_rate": 4.582972444102268e-05,
      "loss": 0.831,
      "step": 5200
    },
    {
      "epoch": 0.2550283899528438,
      "grad_norm": 9.098281860351562,
      "learning_rate": 4.574952683411927e-05,
      "loss": 0.8206,
      "step": 5300
    },
    {
      "epoch": 0.2598402463670484,
      "grad_norm": 2.4031779766082764,
      "learning_rate": 4.566932922721586e-05,
      "loss": 0.8331,
      "step": 5400
    },
    {
      "epoch": 0.264652102781253,
      "grad_norm": 2.636744737625122,
      "learning_rate": 4.5589131620312455e-05,
      "loss": 0.8338,
      "step": 5500
    },
    {
      "epoch": 0.2694639591954576,
      "grad_norm": 2.1573596000671387,
      "learning_rate": 4.550893401340905e-05,
      "loss": 0.8412,
      "step": 5600
    },
    {
      "epoch": 0.2742758156096622,
      "grad_norm": 4.211288928985596,
      "learning_rate": 4.542873640650563e-05,
      "loss": 0.8119,
      "step": 5700
    },
    {
      "epoch": 0.2790876720238668,
      "grad_norm": 7.960838794708252,
      "learning_rate": 4.534853879960222e-05,
      "loss": 0.831,
      "step": 5800
    },
    {
      "epoch": 0.2838995284380714,
      "grad_norm": 3.8395116329193115,
      "learning_rate": 4.526834119269881e-05,
      "loss": 0.8863,
      "step": 5900
    },
    {
      "epoch": 0.288711384852276,
      "grad_norm": 2.568551540374756,
      "learning_rate": 4.51881435857954e-05,
      "loss": 0.816,
      "step": 6000
    },
    {
      "epoch": 0.2935232412664806,
      "grad_norm": 2.988309383392334,
      "learning_rate": 4.510794597889199e-05,
      "loss": 0.8741,
      "step": 6100
    },
    {
      "epoch": 0.2983350976806852,
      "grad_norm": 5.887734889984131,
      "learning_rate": 4.502774837198858e-05,
      "loss": 0.8555,
      "step": 6200
    },
    {
      "epoch": 0.3031469540948898,
      "grad_norm": 4.0593414306640625,
      "learning_rate": 4.494755076508517e-05,
      "loss": 0.8208,
      "step": 6300
    },
    {
      "epoch": 0.3079588105090944,
      "grad_norm": 4.554023265838623,
      "learning_rate": 4.4867353158181765e-05,
      "loss": 0.7974,
      "step": 6400
    },
    {
      "epoch": 0.312770666923299,
      "grad_norm": 5.796566486358643,
      "learning_rate": 4.478715555127836e-05,
      "loss": 0.8134,
      "step": 6500
    },
    {
      "epoch": 0.3175825233375036,
      "grad_norm": 2.651958465576172,
      "learning_rate": 4.470695794437494e-05,
      "loss": 0.8294,
      "step": 6600
    },
    {
      "epoch": 0.3223943797517082,
      "grad_norm": 5.3355607986450195,
      "learning_rate": 4.462676033747153e-05,
      "loss": 0.8054,
      "step": 6700
    },
    {
      "epoch": 0.3272062361659128,
      "grad_norm": 6.539031028747559,
      "learning_rate": 4.454656273056812e-05,
      "loss": 0.8353,
      "step": 6800
    },
    {
      "epoch": 0.3320180925801174,
      "grad_norm": 3.5387773513793945,
      "learning_rate": 4.446636512366471e-05,
      "loss": 0.8657,
      "step": 6900
    },
    {
      "epoch": 0.336829948994322,
      "grad_norm": 3.6193301677703857,
      "learning_rate": 4.43861675167613e-05,
      "loss": 0.8267,
      "step": 7000
    },
    {
      "epoch": 0.3416418054085266,
      "grad_norm": 4.537653923034668,
      "learning_rate": 4.430596990985789e-05,
      "loss": 0.7735,
      "step": 7100
    },
    {
      "epoch": 0.3464536618227312,
      "grad_norm": 2.0860822200775146,
      "learning_rate": 4.422577230295448e-05,
      "loss": 0.8253,
      "step": 7200
    },
    {
      "epoch": 0.3512655182369358,
      "grad_norm": 4.190726280212402,
      "learning_rate": 4.4145574696051075e-05,
      "loss": 0.807,
      "step": 7300
    },
    {
      "epoch": 0.3560773746511404,
      "grad_norm": 4.1563262939453125,
      "learning_rate": 4.406537708914767e-05,
      "loss": 0.8143,
      "step": 7400
    },
    {
      "epoch": 0.360889231065345,
      "grad_norm": 5.026310920715332,
      "learning_rate": 4.398517948224425e-05,
      "loss": 0.8211,
      "step": 7500
    },
    {
      "epoch": 0.3657010874795496,
      "grad_norm": 3.139852285385132,
      "learning_rate": 4.390498187534084e-05,
      "loss": 0.8269,
      "step": 7600
    },
    {
      "epoch": 0.3705129438937542,
      "grad_norm": 3.100567579269409,
      "learning_rate": 4.382478426843743e-05,
      "loss": 0.8521,
      "step": 7700
    },
    {
      "epoch": 0.3753248003079588,
      "grad_norm": 7.26468563079834,
      "learning_rate": 4.374458666153402e-05,
      "loss": 0.8132,
      "step": 7800
    },
    {
      "epoch": 0.3801366567221634,
      "grad_norm": 10.537514686584473,
      "learning_rate": 4.366438905463061e-05,
      "loss": 0.7998,
      "step": 7900
    },
    {
      "epoch": 0.384948513136368,
      "grad_norm": 6.059554100036621,
      "learning_rate": 4.35841914477272e-05,
      "loss": 0.8562,
      "step": 8000
    },
    {
      "epoch": 0.38976036955057264,
      "grad_norm": 4.947580814361572,
      "learning_rate": 4.350399384082379e-05,
      "loss": 0.8043,
      "step": 8100
    },
    {
      "epoch": 0.3945722259647772,
      "grad_norm": 3.497591733932495,
      "learning_rate": 4.3423796233920385e-05,
      "loss": 0.8047,
      "step": 8200
    },
    {
      "epoch": 0.3993840823789818,
      "grad_norm": 3.455538749694824,
      "learning_rate": 4.334359862701698e-05,
      "loss": 0.8175,
      "step": 8300
    },
    {
      "epoch": 0.40419593879318644,
      "grad_norm": 3.3015799522399902,
      "learning_rate": 4.326340102011356e-05,
      "loss": 0.8049,
      "step": 8400
    },
    {
      "epoch": 0.409007795207391,
      "grad_norm": 2.467609405517578,
      "learning_rate": 4.318320341321015e-05,
      "loss": 0.8617,
      "step": 8500
    },
    {
      "epoch": 0.4138196516215956,
      "grad_norm": 5.414824962615967,
      "learning_rate": 4.310300580630674e-05,
      "loss": 0.812,
      "step": 8600
    },
    {
      "epoch": 0.41863150803580024,
      "grad_norm": 5.970311641693115,
      "learning_rate": 4.302280819940333e-05,
      "loss": 0.8274,
      "step": 8700
    },
    {
      "epoch": 0.4234433644500048,
      "grad_norm": 3.032443046569824,
      "learning_rate": 4.294261059249992e-05,
      "loss": 0.8604,
      "step": 8800
    },
    {
      "epoch": 0.4282552208642094,
      "grad_norm": 2.906973361968994,
      "learning_rate": 4.286241298559651e-05,
      "loss": 0.8512,
      "step": 8900
    },
    {
      "epoch": 0.43306707727841404,
      "grad_norm": 4.686370372772217,
      "learning_rate": 4.27822153786931e-05,
      "loss": 0.8091,
      "step": 9000
    },
    {
      "epoch": 0.4378789336926186,
      "grad_norm": 4.591559410095215,
      "learning_rate": 4.2702017771789695e-05,
      "loss": 0.819,
      "step": 9100
    },
    {
      "epoch": 0.4426907901068232,
      "grad_norm": 4.563037395477295,
      "learning_rate": 4.262182016488629e-05,
      "loss": 0.7984,
      "step": 9200
    },
    {
      "epoch": 0.44750264652102784,
      "grad_norm": 4.220534801483154,
      "learning_rate": 4.254162255798287e-05,
      "loss": 0.7862,
      "step": 9300
    },
    {
      "epoch": 0.4523145029352324,
      "grad_norm": 4.386059761047363,
      "learning_rate": 4.246142495107946e-05,
      "loss": 0.8591,
      "step": 9400
    },
    {
      "epoch": 0.457126359349437,
      "grad_norm": 8.185101509094238,
      "learning_rate": 4.238122734417605e-05,
      "loss": 0.8308,
      "step": 9500
    },
    {
      "epoch": 0.46193821576364164,
      "grad_norm": 3.5377285480499268,
      "learning_rate": 4.230102973727264e-05,
      "loss": 0.8544,
      "step": 9600
    },
    {
      "epoch": 0.4667500721778462,
      "grad_norm": 3.1265623569488525,
      "learning_rate": 4.222083213036923e-05,
      "loss": 0.8338,
      "step": 9700
    },
    {
      "epoch": 0.4715619285920508,
      "grad_norm": 6.9490437507629395,
      "learning_rate": 4.214063452346582e-05,
      "loss": 0.8201,
      "step": 9800
    },
    {
      "epoch": 0.47637378500625543,
      "grad_norm": 4.635659217834473,
      "learning_rate": 4.206043691656241e-05,
      "loss": 0.7828,
      "step": 9900
    },
    {
      "epoch": 0.48118564142046,
      "grad_norm": 8.078214645385742,
      "learning_rate": 4.1980239309659005e-05,
      "loss": 0.7928,
      "step": 10000
    },
    {
      "epoch": 0.4859974978346646,
      "grad_norm": 5.621673583984375,
      "learning_rate": 4.19000417027556e-05,
      "loss": 0.835,
      "step": 10100
    },
    {
      "epoch": 0.49080935424886923,
      "grad_norm": 4.345027923583984,
      "learning_rate": 4.181984409585218e-05,
      "loss": 0.7895,
      "step": 10200
    },
    {
      "epoch": 0.4956212106630738,
      "grad_norm": 3.261747360229492,
      "learning_rate": 4.173964648894877e-05,
      "loss": 0.8224,
      "step": 10300
    },
    {
      "epoch": 0.5004330670772784,
      "grad_norm": 4.482031345367432,
      "learning_rate": 4.165944888204536e-05,
      "loss": 0.7992,
      "step": 10400
    },
    {
      "epoch": 0.505244923491483,
      "grad_norm": 5.3708343505859375,
      "learning_rate": 4.157925127514195e-05,
      "loss": 0.8429,
      "step": 10500
    },
    {
      "epoch": 0.5100567799056877,
      "grad_norm": 3.2538039684295654,
      "learning_rate": 4.149905366823854e-05,
      "loss": 0.8038,
      "step": 10600
    },
    {
      "epoch": 0.5148686363198922,
      "grad_norm": 2.7218918800354004,
      "learning_rate": 4.141885606133513e-05,
      "loss": 0.8445,
      "step": 10700
    },
    {
      "epoch": 0.5196804927340968,
      "grad_norm": 4.9781012535095215,
      "learning_rate": 4.133865845443172e-05,
      "loss": 0.8167,
      "step": 10800
    },
    {
      "epoch": 0.5244923491483015,
      "grad_norm": 1.948134422302246,
      "learning_rate": 4.1258460847528315e-05,
      "loss": 0.8275,
      "step": 10900
    },
    {
      "epoch": 0.529304205562506,
      "grad_norm": 3.8275346755981445,
      "learning_rate": 4.117826324062491e-05,
      "loss": 0.8378,
      "step": 11000
    },
    {
      "epoch": 0.5341160619767106,
      "grad_norm": 2.747628688812256,
      "learning_rate": 4.109806563372149e-05,
      "loss": 0.7648,
      "step": 11100
    },
    {
      "epoch": 0.5389279183909153,
      "grad_norm": 3.758033514022827,
      "learning_rate": 4.101786802681808e-05,
      "loss": 0.7825,
      "step": 11200
    },
    {
      "epoch": 0.5437397748051198,
      "grad_norm": 6.617792129516602,
      "learning_rate": 4.093767041991467e-05,
      "loss": 0.8707,
      "step": 11300
    },
    {
      "epoch": 0.5485516312193244,
      "grad_norm": 4.9484477043151855,
      "learning_rate": 4.085747281301126e-05,
      "loss": 0.8708,
      "step": 11400
    },
    {
      "epoch": 0.553363487633529,
      "grad_norm": 3.9914844036102295,
      "learning_rate": 4.0777275206107855e-05,
      "loss": 0.8308,
      "step": 11500
    },
    {
      "epoch": 0.5581753440477336,
      "grad_norm": 4.968400478363037,
      "learning_rate": 4.069707759920444e-05,
      "loss": 0.7999,
      "step": 11600
    },
    {
      "epoch": 0.5629872004619382,
      "grad_norm": 4.805906295776367,
      "learning_rate": 4.061687999230103e-05,
      "loss": 0.8095,
      "step": 11700
    },
    {
      "epoch": 0.5677990568761428,
      "grad_norm": 3.256528377532959,
      "learning_rate": 4.0536682385397625e-05,
      "loss": 0.8086,
      "step": 11800
    },
    {
      "epoch": 0.5726109132903474,
      "grad_norm": 5.064755916595459,
      "learning_rate": 4.045648477849422e-05,
      "loss": 0.8322,
      "step": 11900
    },
    {
      "epoch": 0.577422769704552,
      "grad_norm": 2.6625008583068848,
      "learning_rate": 4.03762871715908e-05,
      "loss": 0.8581,
      "step": 12000
    },
    {
      "epoch": 0.5822346261187566,
      "grad_norm": 5.340049743652344,
      "learning_rate": 4.029608956468739e-05,
      "loss": 0.8076,
      "step": 12100
    },
    {
      "epoch": 0.5870464825329612,
      "grad_norm": 2.5861873626708984,
      "learning_rate": 4.021589195778398e-05,
      "loss": 0.8601,
      "step": 12200
    },
    {
      "epoch": 0.5918583389471658,
      "grad_norm": 7.8170084953308105,
      "learning_rate": 4.013569435088057e-05,
      "loss": 0.7881,
      "step": 12300
    },
    {
      "epoch": 0.5966701953613704,
      "grad_norm": 3.0546493530273438,
      "learning_rate": 4.0055496743977164e-05,
      "loss": 0.8418,
      "step": 12400
    },
    {
      "epoch": 0.601482051775575,
      "grad_norm": 4.1125922203063965,
      "learning_rate": 3.997529913707375e-05,
      "loss": 0.8046,
      "step": 12500
    },
    {
      "epoch": 0.6062939081897796,
      "grad_norm": 2.3608784675598145,
      "learning_rate": 3.989510153017034e-05,
      "loss": 0.8423,
      "step": 12600
    },
    {
      "epoch": 0.6111057646039842,
      "grad_norm": 3.8636741638183594,
      "learning_rate": 3.9814903923266934e-05,
      "loss": 0.7762,
      "step": 12700
    },
    {
      "epoch": 0.6159176210181888,
      "grad_norm": 3.429624557495117,
      "learning_rate": 3.973470631636353e-05,
      "loss": 0.8589,
      "step": 12800
    },
    {
      "epoch": 0.6207294774323934,
      "grad_norm": 4.70973014831543,
      "learning_rate": 3.965450870946011e-05,
      "loss": 0.8101,
      "step": 12900
    },
    {
      "epoch": 0.625541333846598,
      "grad_norm": 3.795301675796509,
      "learning_rate": 3.95743111025567e-05,
      "loss": 0.8231,
      "step": 13000
    },
    {
      "epoch": 0.6303531902608026,
      "grad_norm": 4.537837982177734,
      "learning_rate": 3.949411349565329e-05,
      "loss": 0.8112,
      "step": 13100
    },
    {
      "epoch": 0.6351650466750072,
      "grad_norm": 5.144290447235107,
      "learning_rate": 3.941391588874988e-05,
      "loss": 0.8198,
      "step": 13200
    },
    {
      "epoch": 0.6399769030892118,
      "grad_norm": 3.156893014907837,
      "learning_rate": 3.9333718281846474e-05,
      "loss": 0.7893,
      "step": 13300
    },
    {
      "epoch": 0.6447887595034164,
      "grad_norm": 3.5580461025238037,
      "learning_rate": 3.925352067494306e-05,
      "loss": 0.8248,
      "step": 13400
    },
    {
      "epoch": 0.649600615917621,
      "grad_norm": 3.409771203994751,
      "learning_rate": 3.917332306803965e-05,
      "loss": 0.779,
      "step": 13500
    },
    {
      "epoch": 0.6544124723318256,
      "grad_norm": 7.412744522094727,
      "learning_rate": 3.9093125461136244e-05,
      "loss": 0.8073,
      "step": 13600
    },
    {
      "epoch": 0.6592243287460302,
      "grad_norm": 5.93891716003418,
      "learning_rate": 3.901292785423284e-05,
      "loss": 0.7921,
      "step": 13700
    },
    {
      "epoch": 0.6640361851602348,
      "grad_norm": 4.794755935668945,
      "learning_rate": 3.893273024732942e-05,
      "loss": 0.7941,
      "step": 13800
    },
    {
      "epoch": 0.6688480415744394,
      "grad_norm": 3.4019739627838135,
      "learning_rate": 3.885253264042601e-05,
      "loss": 0.8111,
      "step": 13900
    },
    {
      "epoch": 0.673659897988644,
      "grad_norm": 6.5280842781066895,
      "learning_rate": 3.87723350335226e-05,
      "loss": 0.8258,
      "step": 14000
    },
    {
      "epoch": 0.6784717544028486,
      "grad_norm": 3.742213010787964,
      "learning_rate": 3.869213742661919e-05,
      "loss": 0.783,
      "step": 14100
    },
    {
      "epoch": 0.6832836108170532,
      "grad_norm": 3.85221791267395,
      "learning_rate": 3.8611939819715784e-05,
      "loss": 0.828,
      "step": 14200
    },
    {
      "epoch": 0.6880954672312578,
      "grad_norm": 4.268402576446533,
      "learning_rate": 3.853174221281237e-05,
      "loss": 0.813,
      "step": 14300
    },
    {
      "epoch": 0.6929073236454624,
      "grad_norm": 2.8083527088165283,
      "learning_rate": 3.845154460590896e-05,
      "loss": 0.8516,
      "step": 14400
    },
    {
      "epoch": 0.697719180059667,
      "grad_norm": 5.902036190032959,
      "learning_rate": 3.8371346999005554e-05,
      "loss": 0.8175,
      "step": 14500
    },
    {
      "epoch": 0.7025310364738716,
      "grad_norm": 3.833653450012207,
      "learning_rate": 3.829114939210215e-05,
      "loss": 0.7658,
      "step": 14600
    },
    {
      "epoch": 0.7073428928880762,
      "grad_norm": 4.773346424102783,
      "learning_rate": 3.821095178519873e-05,
      "loss": 0.8182,
      "step": 14700
    },
    {
      "epoch": 0.7121547493022808,
      "grad_norm": 10.373642921447754,
      "learning_rate": 3.813075417829532e-05,
      "loss": 0.848,
      "step": 14800
    },
    {
      "epoch": 0.7169666057164854,
      "grad_norm": 3.4341423511505127,
      "learning_rate": 3.805055657139191e-05,
      "loss": 0.8265,
      "step": 14900
    },
    {
      "epoch": 0.72177846213069,
      "grad_norm": 2.9983408451080322,
      "learning_rate": 3.79703589644885e-05,
      "loss": 0.8088,
      "step": 15000
    },
    {
      "epoch": 0.7265903185448946,
      "grad_norm": 4.684631824493408,
      "learning_rate": 3.7890161357585094e-05,
      "loss": 0.7625,
      "step": 15100
    },
    {
      "epoch": 0.7314021749590992,
      "grad_norm": 4.7975287437438965,
      "learning_rate": 3.780996375068168e-05,
      "loss": 0.8339,
      "step": 15200
    },
    {
      "epoch": 0.7362140313733038,
      "grad_norm": 6.121823310852051,
      "learning_rate": 3.772976614377827e-05,
      "loss": 0.8142,
      "step": 15300
    },
    {
      "epoch": 0.7410258877875084,
      "grad_norm": 4.171667098999023,
      "learning_rate": 3.7649568536874864e-05,
      "loss": 0.862,
      "step": 15400
    },
    {
      "epoch": 0.745837744201713,
      "grad_norm": 6.359307765960693,
      "learning_rate": 3.756937092997146e-05,
      "loss": 0.8067,
      "step": 15500
    },
    {
      "epoch": 0.7506496006159176,
      "grad_norm": 2.680823802947998,
      "learning_rate": 3.748917332306804e-05,
      "loss": 0.7971,
      "step": 15600
    },
    {
      "epoch": 0.7554614570301222,
      "grad_norm": 5.764169692993164,
      "learning_rate": 3.740897571616463e-05,
      "loss": 0.8301,
      "step": 15700
    },
    {
      "epoch": 0.7602733134443268,
      "grad_norm": 5.0572052001953125,
      "learning_rate": 3.732877810926122e-05,
      "loss": 0.8187,
      "step": 15800
    },
    {
      "epoch": 0.7650851698585314,
      "grad_norm": 4.467267990112305,
      "learning_rate": 3.724858050235781e-05,
      "loss": 0.8109,
      "step": 15900
    },
    {
      "epoch": 0.769897026272736,
      "grad_norm": 2.833317756652832,
      "learning_rate": 3.7168382895454404e-05,
      "loss": 0.8336,
      "step": 16000
    },
    {
      "epoch": 0.7747088826869406,
      "grad_norm": 4.074382305145264,
      "learning_rate": 3.708818528855099e-05,
      "loss": 0.7994,
      "step": 16100
    },
    {
      "epoch": 0.7795207391011453,
      "grad_norm": 3.7519946098327637,
      "learning_rate": 3.700798768164758e-05,
      "loss": 0.8136,
      "step": 16200
    },
    {
      "epoch": 0.7843325955153498,
      "grad_norm": 4.241040229797363,
      "learning_rate": 3.6927790074744174e-05,
      "loss": 0.8098,
      "step": 16300
    },
    {
      "epoch": 0.7891444519295544,
      "grad_norm": 3.884450674057007,
      "learning_rate": 3.684759246784077e-05,
      "loss": 0.8201,
      "step": 16400
    },
    {
      "epoch": 0.7939563083437591,
      "grad_norm": 4.089356899261475,
      "learning_rate": 3.6767394860937345e-05,
      "loss": 0.8512,
      "step": 16500
    },
    {
      "epoch": 0.7987681647579636,
      "grad_norm": 3.501763105392456,
      "learning_rate": 3.668719725403394e-05,
      "loss": 0.785,
      "step": 16600
    },
    {
      "epoch": 0.8035800211721682,
      "grad_norm": 3.042752265930176,
      "learning_rate": 3.660699964713053e-05,
      "loss": 0.825,
      "step": 16700
    },
    {
      "epoch": 0.8083918775863729,
      "grad_norm": 3.8257853984832764,
      "learning_rate": 3.652680204022712e-05,
      "loss": 0.7993,
      "step": 16800
    },
    {
      "epoch": 0.8132037340005774,
      "grad_norm": 5.035283088684082,
      "learning_rate": 3.6446604433323714e-05,
      "loss": 0.7736,
      "step": 16900
    },
    {
      "epoch": 0.818015590414782,
      "grad_norm": 4.000754356384277,
      "learning_rate": 3.63664068264203e-05,
      "loss": 0.8222,
      "step": 17000
    },
    {
      "epoch": 0.8228274468289867,
      "grad_norm": 5.182737827301025,
      "learning_rate": 3.628620921951689e-05,
      "loss": 0.8247,
      "step": 17100
    },
    {
      "epoch": 0.8276393032431912,
      "grad_norm": 5.744742393493652,
      "learning_rate": 3.6206011612613484e-05,
      "loss": 0.8063,
      "step": 17200
    },
    {
      "epoch": 0.8324511596573958,
      "grad_norm": 2.9053404331207275,
      "learning_rate": 3.612581400571008e-05,
      "loss": 0.8154,
      "step": 17300
    },
    {
      "epoch": 0.8372630160716005,
      "grad_norm": 6.6459245681762695,
      "learning_rate": 3.6045616398806655e-05,
      "loss": 0.8311,
      "step": 17400
    },
    {
      "epoch": 0.842074872485805,
      "grad_norm": 4.7686357498168945,
      "learning_rate": 3.596541879190325e-05,
      "loss": 0.8095,
      "step": 17500
    },
    {
      "epoch": 0.8468867289000096,
      "grad_norm": 6.578089714050293,
      "learning_rate": 3.588522118499984e-05,
      "loss": 0.7962,
      "step": 17600
    },
    {
      "epoch": 0.8516985853142143,
      "grad_norm": 4.099909782409668,
      "learning_rate": 3.580502357809643e-05,
      "loss": 0.8294,
      "step": 17700
    },
    {
      "epoch": 0.8565104417284188,
      "grad_norm": 3.446366548538208,
      "learning_rate": 3.5724825971193024e-05,
      "loss": 0.792,
      "step": 17800
    },
    {
      "epoch": 0.8613222981426234,
      "grad_norm": 3.164881706237793,
      "learning_rate": 3.564462836428961e-05,
      "loss": 0.8292,
      "step": 17900
    },
    {
      "epoch": 0.8661341545568281,
      "grad_norm": 5.7195916175842285,
      "learning_rate": 3.55644307573862e-05,
      "loss": 0.8264,
      "step": 18000
    },
    {
      "epoch": 0.8709460109710326,
      "grad_norm": 2.313124418258667,
      "learning_rate": 3.5484233150482794e-05,
      "loss": 0.8152,
      "step": 18100
    },
    {
      "epoch": 0.8757578673852372,
      "grad_norm": 3.437856674194336,
      "learning_rate": 3.540403554357939e-05,
      "loss": 0.7996,
      "step": 18200
    },
    {
      "epoch": 0.8805697237994419,
      "grad_norm": 2.85585355758667,
      "learning_rate": 3.5323837936675965e-05,
      "loss": 0.8239,
      "step": 18300
    },
    {
      "epoch": 0.8853815802136464,
      "grad_norm": 3.391066312789917,
      "learning_rate": 3.524364032977256e-05,
      "loss": 0.8019,
      "step": 18400
    },
    {
      "epoch": 0.890193436627851,
      "grad_norm": 3.3776357173919678,
      "learning_rate": 3.516344272286915e-05,
      "loss": 0.8207,
      "step": 18500
    },
    {
      "epoch": 0.8950052930420557,
      "grad_norm": 6.461877822875977,
      "learning_rate": 3.508324511596574e-05,
      "loss": 0.8205,
      "step": 18600
    },
    {
      "epoch": 0.8998171494562602,
      "grad_norm": 3.7878382205963135,
      "learning_rate": 3.5003047509062334e-05,
      "loss": 0.8066,
      "step": 18700
    },
    {
      "epoch": 0.9046290058704648,
      "grad_norm": 2.6277995109558105,
      "learning_rate": 3.492284990215892e-05,
      "loss": 0.8307,
      "step": 18800
    },
    {
      "epoch": 0.9094408622846695,
      "grad_norm": 4.651406764984131,
      "learning_rate": 3.484265229525551e-05,
      "loss": 0.8111,
      "step": 18900
    },
    {
      "epoch": 0.914252718698874,
      "grad_norm": 2.4167068004608154,
      "learning_rate": 3.4762454688352104e-05,
      "loss": 0.8006,
      "step": 19000
    },
    {
      "epoch": 0.9190645751130786,
      "grad_norm": 3.5624938011169434,
      "learning_rate": 3.46822570814487e-05,
      "loss": 0.8082,
      "step": 19100
    },
    {
      "epoch": 0.9238764315272833,
      "grad_norm": 9.535533905029297,
      "learning_rate": 3.4602059474545275e-05,
      "loss": 0.8058,
      "step": 19200
    },
    {
      "epoch": 0.9286882879414878,
      "grad_norm": 4.278465747833252,
      "learning_rate": 3.452186186764187e-05,
      "loss": 0.8232,
      "step": 19300
    },
    {
      "epoch": 0.9335001443556924,
      "grad_norm": 5.388765335083008,
      "learning_rate": 3.444166426073846e-05,
      "loss": 0.8592,
      "step": 19400
    },
    {
      "epoch": 0.9383120007698971,
      "grad_norm": 3.706857681274414,
      "learning_rate": 3.436146665383505e-05,
      "loss": 0.8061,
      "step": 19500
    },
    {
      "epoch": 0.9431238571841016,
      "grad_norm": 3.217041254043579,
      "learning_rate": 3.4281269046931644e-05,
      "loss": 0.8155,
      "step": 19600
    },
    {
      "epoch": 0.9479357135983062,
      "grad_norm": 1.9233310222625732,
      "learning_rate": 3.420107144002823e-05,
      "loss": 0.766,
      "step": 19700
    },
    {
      "epoch": 0.9527475700125109,
      "grad_norm": 2.393437385559082,
      "learning_rate": 3.412087383312482e-05,
      "loss": 0.8612,
      "step": 19800
    },
    {
      "epoch": 0.9575594264267154,
      "grad_norm": 6.522065162658691,
      "learning_rate": 3.4040676226221414e-05,
      "loss": 0.7966,
      "step": 19900
    },
    {
      "epoch": 0.96237128284092,
      "grad_norm": 4.384038925170898,
      "learning_rate": 3.396047861931801e-05,
      "loss": 0.8259,
      "step": 20000
    },
    {
      "epoch": 0.9671831392551247,
      "grad_norm": 2.9855611324310303,
      "learning_rate": 3.3880281012414585e-05,
      "loss": 0.7859,
      "step": 20100
    },
    {
      "epoch": 0.9719949956693292,
      "grad_norm": 2.1533265113830566,
      "learning_rate": 3.380008340551118e-05,
      "loss": 0.7941,
      "step": 20200
    },
    {
      "epoch": 0.9768068520835338,
      "grad_norm": 6.1047844886779785,
      "learning_rate": 3.371988579860777e-05,
      "loss": 0.8157,
      "step": 20300
    },
    {
      "epoch": 0.9816187084977385,
      "grad_norm": 3.707609176635742,
      "learning_rate": 3.363968819170436e-05,
      "loss": 0.7946,
      "step": 20400
    },
    {
      "epoch": 0.986430564911943,
      "grad_norm": 3.6301097869873047,
      "learning_rate": 3.3559490584800954e-05,
      "loss": 0.8181,
      "step": 20500
    },
    {
      "epoch": 0.9912424213261476,
      "grad_norm": 3.8903417587280273,
      "learning_rate": 3.347929297789754e-05,
      "loss": 0.8181,
      "step": 20600
    },
    {
      "epoch": 0.9960542777403523,
      "grad_norm": 6.512176036834717,
      "learning_rate": 3.339909537099413e-05,
      "loss": 0.791,
      "step": 20700
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.6385246493275268,
      "eval_f1": 0.6232215855303119,
      "eval_loss": 0.7951319813728333,
      "eval_precision_macro": 0.6311550199008407,
      "eval_recall_macro": 0.6202863803123795,
      "eval_runtime": 140.9101,
      "eval_samples_per_second": 294.961,
      "eval_steps_per_second": 36.875,
      "step": 20782
    }
  ],
  "logging_steps": 100,
  "max_steps": 62346,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5505807560000256.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
